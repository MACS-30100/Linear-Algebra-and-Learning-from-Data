{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem V.1.1\n",
    "\n",
    "We expect a fraction $p_i$ of the samples to be $X=x_i$. If this is exactly true, $X=x_i$ is repeated $Np_i$ times. \n",
    "\n",
    "#### Problem V.1.2\n",
    "\n",
    "If we add 7 to every output $x$, then the mean will increase by 7. The variance will stay the same. The new expected mean will also increase by 7. \n",
    "\n",
    "#### Problem V.1.3\n",
    "\n",
    "Since divisible by 3 and by 7 are independent of each other, So the fraction of integers can be divisible by both 3 and 7 is: $\\frac{1}{3}\\frac{1}{7} = \\frac{1}{21}$. \n",
    "\n",
    "The fraction of integers will be divisible by either 3 or 7 or both is then $\\frac{1}{3} + \\frac{1}{7} - \\frac{1}{21} = \\frac{9}{21}$\n",
    "\n",
    "#### Problem V.1.4\n",
    "\n",
    "* When the number $n$ is $10,20,\\dot, 100, \\dots, 900, 1000$, we have its last digit as 0. There are 9 numbers ending with 0 from $1~99$, there are 10 numbers ending with 0 for each hundred, e.g. $100~199$ and we also have 1000. So $p_0 = \\frac{1}{1000}(9+10*9+1) = \\frac{100}{1000}$\n",
    "\n",
    "* There are 10 numbers ending with 1 between $[1, 100]$, and $p_1 = \\frac{100}{1000}$\n",
    "So we have $p_i = \\frac{100}{1000} = \\frac{1}{10}$ for $i=0,1,\\dots,9$.\n",
    "\n",
    "The mean of that last digit is thus $m=0\\times \\frac{1}{10} + 1 \\times \\frac{1}{10} + \\dots + 9 \\times \\frac{1}{10} = 4.5$\n",
    "\n",
    "The variance is $\\sigma^2 = 0^2\\times \\frac{1}{10} + 1^2 \\times \\frac{1}{10} + \\dots + 9^2 \\times \\frac{1}{10} = 13.5$\n",
    "\n",
    "#### Problem V.1.5\n",
    "\n",
    "* We see that the number ending with $0$ will still end with $0$ after square. \n",
    "* The numbers ending with $1,9$ will end with $1$ after square.\n",
    "* The numbers ending with $2,8$ will end with $4$ after square.\n",
    "* The numbers ending with $5$ will end with $5$ after square.\n",
    "* The numbers ending with $4,6$ will end with $6$ after square.\n",
    "* The numbers ending with $3,7$ will end with $9$ after square.\n",
    "\n",
    "So we have $p_0 = \\frac{100}{1000} = \\frac{1}{10},p_1=p_4=p_6=p_9 = \\frac{200}{1000} = \\frac{2}{10}, p_5=\\frac{100}{1000} = \\frac{1}{10}$\n",
    "\n",
    "Then the mean $m=(0+5)\\times \\frac{1}{10} + (1 + 4 + 6 + 9)\\times \\frac{2}{10} = 4.5 $\n",
    "The variance $\\sigma^2 = (0+5^2)\\times \\frac{1}{10} + (1^2 + 4^2 + 6^2 + 9^2)\\times \\frac{2}{10} = 29.3$\n",
    "\n",
    "#### Problem V.1.6\n",
    "For $i=2,3,\\dots,9$, there are 1 $i$ between $[1,9]$, there are 10 $i$ between $[10,99]$, there are 100 $i$ between $[i00,i99]$, e.g. $[200,299]$. So in the total there are $100 + 10 + 1 = 111$ for each $i$.\n",
    "There are one extra number starting with 1, which is 1000, so for 1, we have 112 numbers starting with 1.\n",
    "\n",
    "Then we have $p_0 = 0, p_1 = \\frac{112}{1000}$ and $p_i= \\frac{111}{1000}$ for $i=2,\\dots, 9$.\n",
    "\n",
    "The mean $m=1\\times \\frac{112}{1000} + (2+3+\\dots + 9)\\times \\frac{111}{1000} = 4.996$, \n",
    "the variance $\\sigma^2 = 1^2\\times \\frac{112}{1000} + (2^2+3^2+\\dots + 9^2)\\times \\frac{111}{1000} =31.636$\n",
    "#### Problem V.1.7\n",
    "If we have $N=4$ samples $157,312,696,602$, the first digits $x_1=2,x_2=9,x_3=4,x_4=3$ of the squares $24649,97344,484416,362404$. The sample mean $\\mu = \\frac{2+9+4_3}{4} = 4.5$, the sample variance $S^2 = \\frac{(2-4.5)^2+(9-4.5)^2+(4-4.5)^2+(3-4.5)^2}{N-1} = \\frac{29}{3}$\n",
    "\n",
    "#### Problem V.1.8\n",
    "\n",
    "$\\sigma^2 = \\sum p_i(x_i-m)^2 = \\sum p_i (x_i^2 -2mx_i + m^2) = \\sum p_i x_i^2 - 2m\\sum p_i x_i + m^2\\sum_pi = \\sum p_i x_i^2 - 2m^2 + m^2 = \\sum p_i x_i^2 - m^2$\n",
    "\n",
    "#### Problem V.1.9\n",
    "\n",
    "The sample mean $\\mu = 20$, the same variance $S^2 = 0$. If $x=20$ or 21, 12 times each, then we have $\\mu = \\fra{20*12+21*12}{24} = 20.5$, and $S^2 = \\frac{12(20-20.5)^2 + 12(21-20.5)^2}{24-1} = \\frac{6}{23}$\n",
    "\n",
    "#### Problem V.1.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of a million random 0-1 samples:  0.500099\n",
      "The standardized variable X =  4.9499999999980114e-08\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "N = 1000000\n",
    "a = np.random.randint(0,2,N)\n",
    "num_0 = np.sum(a<0.5)\n",
    "num_1 = np.sum(a>=0.5)\n",
    "avg = (num_0 * 0 + num_1 * 1)/N\n",
    "print('Average of a million random 0-1 samples: ', avg)\n",
    "X = (avg-0.5)/2/np.sqrt(N)\n",
    "print('The standardized variable X = ', X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem V.1.11\n",
    "\n",
    "$m= 0p_0 + 1p_1 + \\dots + Np_N = \\sum_i ip_i = \\sum_i \\frac{ib_i}{2^N} = \\frac{1}{2^N}\\sum_i ib_i = \\frac{1}{2^N}\\sum_i i\\frac{N!}{i!(N-i)!} = \\frac{1}{2^N}\\sum_i \\frac{N!}{(i-1)!(N-i)!} = \\frac{N}{2^N}\\sum_i \\frac{(N-1)!}{(i-1)!(N-i)!} = \\frac{N}{2^N}2^{N-1} = \\frac{N}{2}$\n",
    "\n",
    "\n",
    "#### Problem V.1.12\n",
    "\n",
    "$E[x^2] = \\sigma^2 + (E[x])^2 = \\sigma^2 + m^2$\n",
    "\n",
    "#### Problem V.1.13\n",
    "\n",
    "\\begin{align*}\n",
    "2\\pi \\int^{\\infty}_{\\infty}p(x)dx\\int^{\\infty}_{\\infty}p(y)dy &= 2\\pi\\int^{\\infty}_{\\infty}\\int^{\\infty}_{\\infty}p(x)p(y)dxdy \\\\\n",
    "&= 2\\pi\\int^{\\infty}_{\\infty}\\int^{\\infty}_{\\infty}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{y^2}{2}}dxdy \\\\\n",
    "&= \\int^{\\infty}_{\\infty}\\int^{\\infty}_{\\infty}e^{-\\frac{x^2+y^2}{2}}dxdy \\\\\n",
    "&= \\int^{2\\pi}_{0}\\int^{\\infty}_{0}e^{-\\frac{r^2}{2}}rdrd\\theta \\\\\n",
    "&= -\\int^{2\\pi}_{0}e^{-\\frac{r^2}{2}}|^{\\infty}_{0}d\\theta \\\\\n",
    "&= 2\\pi \\\\\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem V.2.1\n",
    "\n",
    "$p_1p_2 = \\frac{1}{2\\pi\\sigma_1\\sigma_2}e^{-\\frac{x^2}{2\\sigma^2_1}-\\frac{x^2}{2\\sigma^2_2}} = \\frac{1}{2\\pi\\sigma_1\\sigma_2}e^{-\\frac{x^2}{2\\frac{\\sigma^2_1\\sigma^2_2}{\\sigma^2_1+\\sigma^2_2}}} = \\frac{1}{2\\pi\\sigma_1\\sigma_2}e^{-\\frac{x^2}{2\\sigma^2}}$\n",
    "\n",
    "It's clear that $\\sigma^2 = \\frac{\\sigma^2_1\\sigma^2_2}{\\sigma^2_1+\\sigma^2_2}$\n",
    "\n",
    "Note, the constant in front of the exponential needs to be scaled to make the integration of $p_1p_2$ equal to 1.\n",
    "\n",
    "#### Problem V.2.2\n",
    "\n",
    "The variance $\\sigma^2_n$ for the convolution of $n$ identical Gaussian $N(0,\\sigma^2)$ is $\\sigma^2_n = \\sigma^2_1 + \\sigma^2_2 + \\dots + \\sigma^2_n$\n",
    "\n",
    "#### Problem V.2.3\n",
    "\n",
    "\\begin{align*}\n",
    "\\int^{\\infty}_{x=-\\infty}P(x)dx &= \\int_x \\int_t p(t)p(x-t)dt \\\\\n",
    "&= \\int_t \\int_x p(t)p(x-t)dt \\\\\n",
    "&= \\int_t p(t) \\\\\n",
    "&= 1\n",
    "\\end{align*}\n",
    "\n",
    "#### Problem V.2.4\n",
    "\n",
    "Suppose $Z=X+Y$, we compute the probability $P(Z\\le z) = P(X+Y\\le z) = \\int_{x+y\\le z} p_x(x)p_y(y)dxdy = \\int_x \\int^{z-x}p_x(x)p_y(y)dxdy$, take derivative w.r.t. $z$ we have the probability density function $p(z) = \\int_x p_x(x)p_y(z-x)dx$      , which is the convolution between $p_x$ and $p_y$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem V.3.1\n",
    "\n",
    "$G(z) = E[z^x] = \\sum z^n p_n = \\sum z^n e^{-\\lambda}\\frac{\\lambda^n}{n!} = e^{-\\lambda}\\sum \\frac{(z\\lambda)^n}{n!} =e^{-\\lambda}e^{z\\lambda}  = e^{(z-1)\\lambda} $\n",
    "\n",
    "#### Problem V.3.2\n",
    "\n",
    "$K_{X+Y}(t) = \\log E[e^{t(X+Y)}] = \\log E[e^{tX}e^{tY}] = \\log E[e^{tX}]E[e^{tY}] = \\log E[e^{tX}] + \\log E[e^{tY}] = K_X(t) + K_Y(t)$\n",
    "\n",
    "#### Problem V.3.3\n",
    "\n",
    "For fair coin, we have $\\bar{X} = 0.5\\times 0 + 0.5\\times 1 = 0.5$, the probability $P(X\\ge 2\\bar{X}) = P(X\\ge 1) = P(X=1) = \\frac{1}{2}$\n",
    "\n",
    "From Markov's inequality, we have with $a=2\\bar{X}$, $P(X\\ge a) \\le \\frac{\\bar{X}}{a} = \\frac{1}{2}$\n",
    "\n",
    "#### Problem V.3.4\n",
    "\n",
    "The actual probability $P(X\\ge 12) = P(X=12) = \\frac{1}{36}$. With $a=12$, we have from Markov's inequality $P(X\\ge 12) \\le \\frac{7}{12}$, which is greater than the probability.\n",
    "\n",
    "#### Problem V.3.5\n",
    "\n",
    "For a nonnegative variable $X$, we have $Y = 0\\le X$ when $X\\le a$, and $Y=a \\lt X$ when $X \\gt a$. So in all cases $Y\\le X$.\n",
    "\n",
    "Then we have $E[Y] = 0\\times P(X\\lt a) + aP(X\\ge a) =a P(X\\ge a)$, also $E[Y] \\le E[X]$, so we have $aP(X\\ge a) \\le E[x]$, so we have $P(X\\ge a) \\le \\frac{E[X]}{a}$.\n",
    "\n",
    "#### Problem V.3.6\n",
    "\n",
    "For two symmetric matrices $S$ and $T$, we have $\\lambda_{max}(S+T) \\le \\lambda_{max}(S) + \\lambda_{max}(T)$ according to the equation (15) on page 172 from \"III.2 Interlacing Eigenvalues and Low Rank Signals\". \n",
    "\n",
    "So we have $\\lambda_{max}(\\bar{Y}) = \\lambda_{max}\\left(p_1Y_1+\\dots + p_nY_n \\right) \\le \\lambda_{max}(p_1Y_1) + \\dots \\lambda_{max}(p_nY_n) = p_1\\lambda_{max}(Y_1) + \\dots + p_n\\lambda_{max}(Y_n) = E[\\lambda_{max}(Y)]$\n",
    "\n",
    "#### Problem V.3.7\n",
    "\n",
    "$A-B = \\begin{bmatrix}1 & 0 \\\\ 0 & 0 \\end{bmatrix}$, this is positive semidefinite because it has 0 and 1 as eigenvalues.\n",
    "\n",
    "$A^2 - B^2 = \\begin{bmatrix}5 & 3 \\\\ 3 & 2\\end{bmatrix} - \\begin{bmatrix}2 & 2 \\\\ 2 & 2\\end{bmatrix} = \\begin{bmatrix}3 & 1 \\\\ 1 & 0\\end{bmatrix}$\n",
    "The determinant is $-1$ which is less than 0, so it's not positive semidefinite.\n",
    "\n",
    "\n",
    "#### Problem V.3.8\n",
    "\n",
    "When $t \\le x_1$, $P(x>t) = 1$, if $x_1 < t < x_2$, $P(x>t) = 1 - p_1$, if $x_i < t < x_{i+1}$, then $P(x>t) = 1 - \\sum^{i}_{k=1}p_k$\n",
    "\n",
    "\\begin{align*}\n",
    "\\int^{\\infty}_{t=0} P(x>t)dt &= \\int^{x_1}_{t=0} P(x>t)dt + \\int^{x_2}_{x_1}P(x>t)dt + \\dots +  \\int^{x_{i+1}}_{x_i}P(x>t)dt + \\dots + \\int^{\\infty}_{x_n}P(x>t)dt\\\\\n",
    "&= 1x_1 + (1-p_1)(x_2-x_1) + \\dots + (1-\\sum^{i-1}_{k=1}p_k)(x_{i}-x_{i-1}) + (1-\\sum^{i}_{k=1}p_k)(x_{i+1}-x_i) + \\dots + p_n(x_n - x_{n-1}) + 0\\\\\n",
    "&= p_1x_1 + p_2x_2 + \\dots + p_nx_n \\\\\n",
    "&= \\sum^n_{i=1}p_ix_i\\\\\n",
    "&= E[x]\\\\\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem V.4.1\n",
    "\n",
    "* (a) The mean $m = p\\times 1 + (1-p)\\times 0 = p$, the variance $\\sigma^2 = p(1-p)^2 + (1-p)(0-p)^2 = p(1-p)$\n",
    "\n",
    "* (b) The variance of a sum is the sum of variance for each flip.\n",
    "\n",
    "#### Problem V.4.2\n",
    "\n",
    "$\\sigma_{53} = \\sigma_{35} = \\sum^n_{i=1}\\sum^n_{j=1} p_{ij}(x_i - m_x)(y_j-m_y)$\n",
    "\n",
    "#### Problem V.4.3\n",
    "\n",
    "$V=\\sum\\sum\\sum p_{ijk} \\begin{bmatrix} (x_i-m_1)^2 & (x_i-m_1)(y_j-m_2) & (x_i - m_1)(z_k-m_3) \\\\ (x_i-m_1)(y_j-m_2) & (y_j-m_2)^2 & (y_j-m_2)(z_k-m_3) \\\\ (x_i - m_1)(z_k-m_3) & (y_j-m_2)(z_k-m_3) & (z_k - m_3)^2\\end{bmatrix}$\n",
    "\n",
    "#### Problem V.4.4\n",
    "\n",
    "Independent experiments have covariance 0, so we have \n",
    "\n",
    "$V=\\begin{bmatrix}\\sigma^2_1 & 0 & 0 \\\\ 0 & \\sigma^2_2 & 0 \\\\ 0 & 0 & \\sigma^2_3\\end{bmatrix}$\n",
    "\n",
    "#### Problem V.4.5\n",
    "\n",
    "Notice that the mean of a linear combination of variables is the linear combination of the means of the variables, i.e. $\\overline{AX} = A\\overline{X}$.\n",
    "\n",
    "$Z=E\\left[(AX-\\overline{AX})(AX-\\overline{AX})^T\\right]= E\\left[(AX-A\\overline{X})(AX-A\\overline{X})^T\\right] = E\\left[A(X-\\overline{X})(X-\\overline{X})^TA^T\\right] = AE\\left[(X-\\overline{X})(X-\\overline{X})^T\\right]A^T$\n",
    "\n",
    "Where in the last step we used the fact that the expectation of linear combination of variables are linear combination of the expectation of variables. \n",
    "\n",
    "#### Problem V.4.6\n",
    "\n",
    "The probability of $Y=y_j$ can be thought of the sum over all outputs $x_i$ of the conditional probability $P(Y=y_j|X=x_i)$. \n",
    "$P(Y=y_j) = \\sum_i P(Y=y_j|X=x_i)P(X=x_i)$\n",
    "\n",
    "#### Problem V.4.7\n",
    "\n",
    "The joint probability $p_{ij} = P(X=x_i \\cap Y=y_j) = P(Y=y_j|X=x_i)P(X=x_i) = P(Y=y_j|X=x_i)p_i$, so we have $P(Y=y_j|X=x_i) = \\frac{p_{ij}}{p_i}$\n",
    "\n",
    "#### Problem V.4.8\n",
    "\n",
    "$p(x_1) = p_{11} + p_{12} = 0.1 + 0.3 = 0.4$\n",
    "\n",
    "$p(y_2|x_1) = \\frac{p(y_2,x_1)}{p(x_1)} = \\frac{p_{12}}{p_1} = \\frac{0.3}{0.4} = 0.75$\n",
    "\n",
    "#### Problem V.4.9\n",
    "\n",
    "The joint probability $p_{ij}$ means that $X=x_i$, $Y=y_j$ are both happending, if we fix $X=x_i$ first, then consider the conditional probability that given $X=x_i$, the probability of $Y=y_j$, i.e. $P(Y=y_j|X=x_i)$, this even is independent of $X=x_i$, so we can multiply them together to get the join probability.\n",
    "\n",
    "#### Problem V.4.10\n",
    "\n",
    "From problem 9, we have $P(Y=y_j|X=x_i) = \\frac{P(Y=y_j\\cap X=x_i)}{P(X=x_i)} = \\frac{P(X=x_i|Y=y_j)P(Y=y_j)}{P(X=x_i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem V.5.1\n",
    "\n",
    "Write down the two equations as $Ax=b$, we have: $\\begin{bmatrix}1 \\\\ 1\\end{bmatrix}\\begin{bmatrix}x\\end{bmatrix} = \\begin{bmatrix}b_1 \\\\ b_2\\end{bmatrix}$, the covariance matrix for the measurements is $V=\\begin{bmatrix}\\sigma^2_1 & 0 \\\\ 0 & \\sigma^2_2 \\end{bmatrix}$. \n",
    "\n",
    "Multiply $A\\hat{x}=b$ by $V^{-\\frac{1}{2}}$ on both sides, we have $A^TV^{-1}A\\hat{x} = A^TV^{-1}b$, i.e. \n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}1 & 1 \\end{bmatrix} \\begin{bmatrix}\\frac{1}{\\sigma^2_1} & 0\\\\ 0 & \\frac{1}{\\sigma^2_2}\\end{bmatrix}\\begin{bmatrix}1 \\\\ 1\\end{bmatrix}\\hat{x} &= \\begin{bmatrix}1 & 1 \\end{bmatrix} \\begin{bmatrix}\\frac{1}{\\sigma^2_1} & 0\\\\ 0 & \\frac{1}{\\sigma^2_2}\\end{bmatrix}\\begin{bmatrix}b_1 \\\\ b_2 \\end{bmatrix}\\\\\n",
    "(\\frac{1}{\\sigma^2_1}+\\frac{1}{\\sigma^2_2})\\hat{x} &= \\frac{b_1}{\\sigma^2_1}+\\frac{b_2}{\\sigma^2_2}\\\\\n",
    "\\hat{x} &= \\frac{\\frac{b_1}{\\sigma^2_1}+\\frac{b_2}{\\sigma^2_2}}{\\frac{1}{\\sigma^2_1}+\\frac{1}{\\sigma^2_2}}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Then we can compute the variance of $\\hat{x}$, i.e. $W=(A^TV^{-1}A)^{-1}=(\\frac{1}{\\sigma^2_1}+\\frac{1}{\\sigma^2_2})^{-1}$\n",
    "\n",
    "#### Problem V.5.2\n",
    "\n",
    "* (a) if $\\sigma_2 \\to 0$, then we have $\\hat{x} = b_2$, which makes $\\hat{x}$ the exact measurement due its high accuracy.\n",
    "\n",
    "* (b) If $\\sigma_2 \\to \\infty$, then we have $\\hat{x} = b_1$. The $2$nd measurement is so unreliable, we just use $b_1$ here to estimate.\n",
    "\n",
    "#### Problem V.5.3\n",
    "\n",
    "* $\\int\\int p(x,y)dxdy = \\int \\int p(x)p(y)dxdy=\\int_x p(x)dx \\int_y p(y)dy = \\int_x p(x)dx = 1$\n",
    "\n",
    "* $\\int\\int (x+y)p(x,y)dxdy = \\int\\int (x+y)p(x)p(y)dxdy = \\int_x xp(x) \\int_y p(y)dy dx + \\int_y yp(y) \\int_x p(x)dx dy = \\int_x xp(x)dx + \\int_y yp(y)dy = m_1 + m_2 $.\n",
    "\n",
    "\n",
    "#### Problem V.5.4\n",
    "\n",
    "* $\\int \\int (x-m_1)^2p(x,y)dxdy = \\int \\int (x^2 -2m_1x + m^2_1)p(x)p(y)dxdy = \\int_x x^2p(x) \\int_y p(y) dy dx - 2m_1 \\int_x xp(x) \\int_y p(y)dy dx + m^2_1 \\int \\int p(x,y)dxdy = \\int_x x^2p(x)dx - 2m_1\\int_x xp(x)dx + m^2_1 = \\int_x x^2p(x)dx - 2m_1\\int_x xp(x)dx + \\int_x m^2_1p(x)dx =  \\int_x (x-m_1)^2p(x)dx = \\sigma^2_1$\n",
    "\n",
    "* $\\int\\int (x-m_1)(y-m_2)p(x,y)dxdy = \\int\\int xyp(x,y)dxdy - m_2\\int\\int xp(x,y)dxdy - m_1\\int\\int yp(x,y)dxdy + m_1m_2\\int\\int p(x,y) dxdy = \\int_x xp(x) \\int_y yp(y)dy dx - m_2m_1 - m_1 m_2 + m_1 m_2 = m_1m_2 - m_1m_2 = 0$\n",
    "\n",
    "#### Problem V.5.5\n",
    "\n",
    "* $\\hat{x}_{k+1} = \\hat{x}_k + \\frac{1}{k+1}(b_{k+1} - \\hat{x}_k) =\\frac{k}{k+1}\\hat{x}_k +  \\frac{1}{k+1}b_{k+1}=\\frac{k}{k+1}\\frac{1}{k}\\sum^k_{i=1}b_i + \\frac{1}{k+1}b_{k+1} = \\frac{1}{k+1}\\sum^k_{i=1}b_i + \\frac{1}{k+1}b_{k+1} =\\frac{1}{k+1}\\sum^{k+1}_{i=1}b_i$\n",
    "\n",
    "* $W_k = \\frac{\\sigma^2}{k}$, also $V_{k+1} = \\sigma^2$, so we have $W^{-1}_{k+1} = W^{-1}_k + A^T_{k+1}V^{-1}_{k+1}A_{k+1} = \\frac{k}{\\sigma^2} + \\frac{1}{\\sigma^2} = \\frac{k+1}{\\sigma^2}$, in the end we have $W^{k+1} = \\frac{\\sigma^2}{k+1}$\n",
    "\n",
    "Where $A_{k+1}=I$\n",
    "\n",
    "#### Problem V.5.6\n",
    "\n",
    "Divide both sides by $\\sigma, s, \\sigma$, we have $\\begin{bmatrix} \\frac{1}{\\sigma} & 0 \\\\ -\\frac{1}{s} & \\frac{1}{s} \\\\ 0 & \\frac{1}{\\sigma} \\end{bmatrix} \\begin{bmatrix} x_0 \\\\x_1 \\end{bmatrix} = \\begin{bmatrix}\\frac{b_0}{\\sigma} \\\\ 0 \\\\ \\frac{b_1}{\\sigma}\\end{bmatrix}$, then we have matrix $A=\\begin{bmatrix}1 & 0 \\\\ -1 & 1 \\\\ 0 & 1\\end{bmatrix}$, $V^{-\\frac{1}{2}}=\\begin{bmatrix}\\frac{1}{\\sigma} & 0 & 0\\\\ 0 & \\frac{1}{s} & 0 \\\\ 0 & 0 & \\frac{1}{\\sigma}\\end{bmatrix}$\n",
    "\n",
    "And we need solve $\\hat{x}$ from this equation $A^TV^{-1}A\\hat{x} = A^TV^{-1}b$, that is \n",
    "\n",
    "$\\begin{bmatrix}1 & -1 & 0 \\\\ 0 & 1 & 1\\end{bmatrix} \\begin{bmatrix}\\frac{1}{\\sigma^2} & 0 & 0\\\\ 0 & \\frac{1}{s^2} & 0 \\\\ 0 & 0 & \\frac{1}{\\sigma^2}\\end{bmatrix}\\begin{bmatrix}1 & 0 \\\\ -1 & 1 \\\\ 0 & 1\\end{bmatrix} \\hat{x} =\\begin{bmatrix}1 & -1 & 0 \\\\ 0 & 1 & 1\\end{bmatrix} \\begin{bmatrix}\\frac{1}{\\sigma^2} & 0 & 0\\\\ 0 & \\frac{1}{s^2} & 0 \\\\ 0 & 0 & \\frac{1}{\\sigma^2}\\end{bmatrix} \\begin{bmatrix}b_0 \\\\ 0 \\\\ b_1\\end{bmatrix}$, i.e. \n",
    "\n",
    "$ \\begin{bmatrix} \\frac{1}{\\sigma^2} + \\frac{1}{s^2} & -\\frac{1}{s^2} \\\\ -\\frac{1}{s^2} &  \\frac{1}{\\sigma^2} + \\frac{1}{s^2}\\end{bmatrix} \\hat{x} = \\begin{bmatrix} \\frac{b_0}{\\sigma^2} \\\\ \\frac{b_1}{\\sigma^2} \\end{bmatrix}$\n",
    "\n",
    "So we have $\\hat{x} = \\frac{\\sigma^2}{\\frac{1}{\\sigma^2}+\\frac{2}{s^2}}\\begin{bmatrix}\\frac{1}{\\sigma^2}+ \\frac{1}{s^2}& \\frac{1}{s^2} \\\\ \\frac{1}{s^2}& \\frac{1}{\\sigma^2}+ \\frac{1}{s^2}\\end{bmatrix}\\begin{bmatrix} \\frac{b_0}{\\sigma^2} \\\\ \\frac{b_1}{\\sigma^2} \\end{bmatrix} = \\frac{1}{\\frac{1}{\\sigma^2}+\\frac{2}{s^2}}\\begin{bmatrix}b_0(\\frac{1}{\\sigma^2}+ \\frac{1}{s^2}) + b_1\\frac{1}{s^2}\\\\ b_0\\frac{1}{s^2} + b_1(\\frac{1}{\\sigma^2}+ \\frac{1}{s^2})\\end{bmatrix}$\n",
    "\n",
    "So it says $\\hat{x}_0$ has more weight to $b_0$, while $\\hat{x}_1$ has more weight to $b_1$.\n",
    "\n",
    "This is exactly the same as problem III.1.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
